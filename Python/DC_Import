PEP8 , by Guido van Rossum, BDFL
PEP20 Tim Peters, 20 aphorisms:   >import this
#### Magic command
!ls

filename = 'huck_finn.txt'
file = open(filename , mode='r')    # 'r' is to read
file = open(filename , mode='w')    # 'w' is to read
text = file.read()
file.close()
print(text)

# Context managers will automatoicall close files, and bind variable with the open statement
with open(filename, mode='r') as file:
    print(file.read())
    
# Read & print the first 3 lines
with open('moby_dick.txt') as file:
    print(file.readline())
    print(file.readline())
    print(file.readline())
    

# row , record 
# columns =fields or attributes or features

########## Using Numpy ########## loadtxt() , genfromtxt() , recfromcsv()  
Numpy can import arrays of all types, or can auto-interpret into structured array (genfromtxt/recfromcsv)

import numpy as np
filename= 'MNIST.txt'
data=np.loadtxt(filename, delimiter=',')  # delimiter is whitespace by default
data=np.loadtxt(filename, delimiter=',' , skiprows=1, usecols=[0,2])
data=np.loadtxt(filename, delimiter=',' , skiprows=1, usecols=[0,2], dtype=str)  # import all data as str
data=np.loadtxt(filename, delimiter=',' , skiprows=1, usecols=[0,2], dtype=float)  # all data are floats
print(data[9])   # Prints 10th line

#mixed data types  structured array... 1D arraw , where each element of arraw is a row of the flat file
data = np.genfromtxt(filename, delimiter=',', names=True, dtype=None)    # names=True if header, dtype=None for auto

file = 'titanic.csv'
data=np.recfromcsv(file)  #  dtype=None, delimiter=',' by default
print(data[:3])   # prints out first three enteries of d

##### Example Import package
import numpy as np

# Assign filename to variable: file
file = 'digits.csv'

# Load file as array: digits
digits = np.loadtxt(file, delimiter=',')

# Print datatype of digits
print(type(digits))

# Select and reshape a row
im = digits[21, 1:]
im_sq = np.reshape(im, (28, 28))

# Plot reshaped data (matplotlib.pyplot already loaded as plt)
plt.imshow(im_sq, cmap='Greys', interpolation='nearest')
plt.show()

# Assign filename: file
file = 'seaslug.txt'

###### Example Import file: data
data = np.loadtxt(file, delimiter='\t', dtype=str)

# Print the first element of data
print(data[0])

# Import data as floats and skip the first row: data_float
data_float = np.loadtxt(file, delimiter='\t', dtype=float, skiprows=1)

# Print the 10th element of data_float
print(data_float[9])

# Plot a scatterplot of the data
plt.scatter(data_float[:, 0], data_float[:, 1])
plt.xlabel('time (min.)')
plt.ylabel('percentage of larvae')
plt.show()

########## Using Pandas ##########
# standard best practice to put data into data frames with pandas
import pandas as pd
filename = 'wine-quality.csv'
data = pd.read_csv(file)   # delimiter default=,   header=True by default
data2 = pd.read_csv(file, nrows=5, header=None)   
data3 = pd.read_csv(file, sep='\t' , comment='#', na_values='Nothing')
data.head()
data_array=data.values          # transfers dataframe into numpy array
print(type(data_array)

pd.DataFrame.hist(data[['Age']])
plt.xlabel('Age (years)')
plt.ylabel('count')
plt.show()

#### keep eye on Feather , made by HadleyWickham and wesmckinney

##### Pickled files  (serialized)
import pickle
with open('pickled_fruit.pkl', 'rb') as file:       # 'rb' read, binary
    data=pickle.load(file)
print(data)
print(type(data))

##### Excel Sheets
import pandas as pd
file='urbanpop.xls'
data= pd.ExcelFile(file)
print(data.sheet_names)
df1 = data.parse('sheetname')   # text string for sheet
df2 = data.parse(0)             # sheet index [0,1,2, .. etc
df3 = data.xl.parse(0,skiprows=1, names=['Country', 'AAM'])        # parse 1st sheet  and rename the columns
df4 = data.parse(0,parse_cols[0], skiprows=1, names=['Country'])   # parse 1st column, rename to country
print(df2.head())

##### SAS / Stata 
import pandas as pd
from sas7bdat import SAS7BDAT
with SAS7BDAT('urbanpop.sas7bdat') as file:
    df_sas=file.to_data_frame()
pd.DataFrame.hist(df_sas[['P']])         # Print a histogram of one column of the DataFrame
plt.ylabel('count')
plt.show()

import pandas as pd
df = pd.read_stata('urbanpop.dta')
print(df.head())
pd.DataFrame.hist(df[['disa10']])        # Print a histogram of one column of the DataFrame
plt.xlabel('Extent of disease')
plt.ylabel('Number of countries')
plt.show()

##### HDF5 files     ..can scale to exabytes
import h5py
filename = 'H-H1_LOSC_4_V1-81541100-4096.hdf5'
data= h5py.File(filename, 'r')   # 'r' for read
print(type(data)

for key in data.keys():
    print(key)                   #  meta , quality , strain
print(type(data['meta']))        # <class 'h5py._hl.group.Group'>
for key in data['meta'].keys()
    print(key)                   # Description , DescritptionURL , Detector , Duration ....
print(data['meta']['Description'].value,data['meta']['Detector'].value)   # b'Strain data'

group = data['strain']          # Get the HDF5 group: group
for key in group.keys():        # Check out keys of group
    print(key)

# Example
strain = data['strain']['Strain'].value     # Set variable equal to time series data: strain
num_samples = 10000                         # Set number of time points to sample: num_samples
time = np.arange(0, 1, 1/num_samples)       # Set time vector
plt.plot(time, strain[:num_samples])        # Plot data
plt.xlabel('GPS Time (s)')
plt.ylabel('strain')
plt.show()


### Matlab files ... .mat files  containers for various objects
scipy.io.loadmat()       scipy.io.savemat()  

import scipy.io
filename = 'workspace.mat'
mat = scipy.io.loadmat(filename)
print(type(mat))            # <class 'dict'>
print((type(mat['x'}))      # <class 'numpy.ndarray'>

print(mat.keys())
print(type(mat['CYratioCyt']))         # print the type corresponding to key CYratioCyt
print(np.shape(mat['CYratioCyt']))     # print the shape of hte value 
data = mat['CYratioCyt'][25, 5:]
fig = plt.figure()
plt.plot(data)
plt.xlabel('time (min.)')
plt.ylabel('normalized fluorescence (measure of expression)')
plt.show()


########## Relational Databases ##########    sqlite3  sqlalchemy       ... Todd's 12 Rules (0indexed = 13)
#####  PostgreSQL , SQLite , mysql , DB2 , SASql , HiveSQL , Structured Query Language
from sqlalchemy import create_engine
engine = create_engine('sqlite:///Northwind.sqlite')
table_names = engine.table_names()
print(table_names)

SELECT * FROM Table_Name    (Hello World of SQL)

from sqlalchemy import create_engine
import pandas as pd
engine = create_engine('sqlite:///Northwind.sqlite')

con = engine.connect()
rs = con.execute("SELECT * FROM Orders")
df = pd.DataFrame(rs.fetchall())
df.columns = rs.keys()
con.close()

with engine.connect() as con:
    rs = con.execute("SELECT OrderId, OrderDate, FROM Orders")
    df = pd.dataFrame(rs.fecthmany(size=5))
    df.columns = rs.keys()
print(len(df))


## import with pandas
sqltext ="SELECT Title , Name FROM Album INNER JOIN Artist ON Album.ArtistID = Artist.ArtistID WHERE Mills<250000"
df = pd.read_sql_query(sqltext, engine)




########################## Web Data #######

from urllib.request import urlretrieve                            # similar to urlopen() , but accepts URLs
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
urlretrieve(url, 'winequality-white.csv')           # note, saves the .csv localy  (makes GET request, and local save)


from urllib.request import urlretrieve
import pandas as pd
url='https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'
urlretrieve(url,'winequality-red.csv')
df = pd.read_csv('winequality-red.csv', sep=';')       # Read local file into a DataFrame and print its head
print(df.head())

df = pd.read_csv(url, sep=';')                         # Read url directly into a DataFrame and print its head
print(df.head())

pd.DataFrame.hist(df.ix[:, 0:1])                             # Plot first column of df
plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')
plt.ylabel('count')
plt.show()



#### Importing url to panda dictionary without local data save , even with excel data
import pandas as pd                    # Import package
url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'
xl = pd.read_excel(url,sheetname=None)  # Read in all sheets of Excel file: xl , sheets are keys
print(type(xl))                         # <class 'dict'>
print(xl.keys())                        # Print the sheetnames to the shell
print(xl['1700'].head())                # Print the head of the first sheet (using its name, NOT its index)



##### getting HTTP GET
from urllib.request import urlopen, Request
url = 'https://www.wikipedia.org/'
reqeust = Reqeust(url)
response = urlopen(request)
html = response.read()
print(type(html))
response.close()

# better automated using the Requests package
import requests
url = 'https://www.wikipedia.org/'
r= reqeusts.get(url)
text = r.text


from bs4 import BeautifulSoup
import requsts
url = 'https://www.crummy.com/software/BeautifulSoup/'
r= requsts.get(url)
html_doc = r.text
soup = BeautifulSoup(html_doc)
print(soup.title)
print(soup.get_text())
a_tags = soup.find_all('a')
for link in a_tags:                 # Print the URLs to the shell
    print(link.get('href'))

for link in soup.find_all('a'):      # shortcut
    print(link.get('href'))


pretty_soup = BeautifulSoup.prettify(soup)


#####  JSON (JavaScript Object Notation (Douglas Crokford) .. human readable real-time server-to-browser communication
import json
with open('snakes.json' , 'r') as json_file:
    json_data = json.load(json_file)
print(type(json.data))
for k in json_data.keys():
    print(k + ': ', json_data[k])    


import requests
url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'
url = 'http://www.omdbapi.com/?t=hackers&apikey=ff21610b'      # ? is a query string .. text follwoing is the query
url = 'http://www.omdbapi.com/?apikey=ff21610b&t=the+social+network'          # note, apikey needs to be included sometimes
r= reqeusts.get(url)
json_data = r.json()                # built in json method!
for key, value in json_data.items():
    print(key + ':' , value)

# some json reults are nested jsons ... like wikipedia on pizza
pizza_extract = json_data['query']['pages']['24768']['extract']
print(pizza_extract)


##### Twitter API and authentification     (note, need to login and get tokens, and create Twitter App)
import tweepy, json
access_token        = "..."
access_token_secret = "..."
consumer_key        = "..."
consumer_secret     = "..."
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)

# define stream listener class
st_class.py
--------
class MyStreamListner(tweepy.StreamListener):
    def __init__(self, api=None):
        super(MyStremListener, self).__init__()
        self.num_tweets = 0
        self.file = open("tweets.txt","w")
        
    def on_status(self, status):
        tweet = status._json
        self.file.write(json.dumps(tweet) + '\n')
        tweet_list.append(status)
        self.num_tweets += 1
        if self.num_tweets < 100:
            return True
        else:
            return False
        self.file.close()
   
l= MystreamListener()           # create streaming object and authenticate 
stream tweepy.Stream(auth,l)

stream.filter(track=['apples','oranges'])        # filters Twitter Streams to capture by keyword
        

#### Example Import package 
import tweepy


# Load and explore twitter data (from file on path)
import json
tweets_data_path = 'tweets.txt'
tweets_data = []                                    # Initialize empty list to store tweets: tweets_data, list of dictionaries
tweets_file = open(tweets_data_path, "r")           # Open connection to file
for line in tweets_file:                            # Read in tweets and store in list: tweets_data
    tweet = json.loads(line)                        # Each tweet is a dictionary
    tweets_data.append(tweet)                       # append to the list of dictionaries

tweets_file.close()                                  # Close connection to file


### Twitter to data Frame   ... make each row a tweet , and 2 columns, text and lang, from t['text'] and t['lang']
import pandas as pd
df = pd.DataFrame(tweets_data,['text','lang']            #  from list of dictionaries, extract the columns listed in list
print(df.head())

# Initialize list to store tweet counts
[clinton, trump, sanders, cruz] = [0, 0, 0, 0]

# Iterate through df, counting the number of tweets in which
# each candidate is mentioned
for index, row in df.iterrows():
    clinton += word_in_text('clinton', row['text'])
    trump   += word_in_text('trump'  , row['text'])
    sanders += word_in_text('sanders', row['text'])
    cruz    += word_in_text('cruz'   , row['text'])


import seaborn as sns
import matplotlib.pyplot as plt

sns.set(color_codes=True)                              # Set seaborn style

cd = ['clinton', 'trump', 'sanders', 'cruz']           # Create a list of labels:cd

ax = sns.barplot(cd, [clinton,trump,sanders,cruz])     # Plot histogram
ax.set(ylabel="count")
plt.show()


####################################  Intro to Databases ####################################
# Secquence strings:   'Driver+Dialect:///Filename'
# Reflection reads database and builds SQLAlchemy Table objects automatically, without lots of handwork

from sqlalchemy import create_engine                     # note db2 is an external dialect http://docs.sqlalchemy.org/en/latest/dialects/
engine = create_engine('sqlite://census_nyc.sqlite')     # https://github.com/ibmdb/python-ibmdb
connection = engine.connect()
print(engine.table_names())                             # 'census' , ''

from sqlalchemy import MetaData, Table              # Metadta object is a catalog storing database info (eg tables)
metadata = MetaData()                               # Reflects the data by initializing a Metadata object
census = Table('census', metadata, autoload=True, autoload_with=engine)    #
print(repr(census))                                 # allows us to see names of columns and column types

print(census.columns.keys())                        # Pritn column names
print(repr(metadata.tables['census']))              # Print full table metadata


##### basic sql querying
from sqlalchemy import create_engine
engine = create_engine('sqlite:///census_nyc.sqlite')
connection = engine.connect()
stmt = 'SELECT * FROM people'
result_proxy = connection.execute(stmt)             # obtains ReslutProxy object, 
results = results_proxy.fetchall()                  # results will contain all the data queried in the ResultSet

first_row= results[0]
print(first_row)            # ('Illinois', 'M', 0, 89600, 95012
print(first_row.keys())     # ['state' , 'sex' , 'age', 'pop2000' , 'pop2008']
print(first_row.state)      # 'Illinois'
print(first_row['state'])   # 'Illinois'


### SQLAlchemy querying
from sqlalchemy import Table, MetaData, select
engine = create_engine('sqlite:///census_nyc.sqlite')
connection = engine.connect()
metadata = MetaData()
census = Table('census', metadata, autoload=True, autoload_with=engine)    #
stmt = select([census])                                                # selects all the rows and columns of census table
results = connection.execute(stmt).fecthall()                          # Combines ResultProxy and ResultSet obtain all the results
print(stmt)                                                             # 'SELECT * from CENSUS'

print(connection.execute(stmt).fetchall())


#### Tailoring statemennts
stmt = select([census])
stmt = stmt.where(census.columns.state == 'California')      # >= <= , != , ==   in_()  like()  between()
results = connection.execute(stmt).fetchall()
for result in results:
    print(result.state , result.age)

stmt = select([census])
stmt = stmt.where(census.columns.state.startswith('New')      # lots of other functions
for result in connection.execute(stmt):               # note, no fecthmethod ... allows ResultProxy to be used as target of loop
    print(result.state , result.pop2000)

from sqlalchemy import or_
stmt = select([census])
stmt = stmt.where( or_(census.columns.state =='New York',census.columns.state =='California'))      # Conjunctions and_() not_() or_()
results = connection.execute(stmt).fetchall()
for result in results:
    print(result.state , result.sex)

#### Connecting to PostgreSQL  ... perhaps use psycopg2 database driver to fully facilitate all native functions
from sqlalchemy import create_engine
driver_dialect = 'postgresql+psycopg2://'
userpass = 'student:datacamp'
hostport = '@postgresql.csrrinzqubik.us-east-1.rds.amazonaws.com:5432/'
database = 'census' 
dbase = driver_dialect + userpass + hostport + database
engine = create_engnie(dbase)
print(engine.table_names())                                             # ['census', 'state_fact', 'data', 'users']
##  'postgresql+psycopg2://student:datacamp@postgresql.csrrinzqubik.us-east-1.rds.amazonaws.com:5432/census' 

stmt = select([census])
stmt = stmt.where(census.columns.state == 'New York')
results = connection.execute(stmt).fetchall()
for result in results:
    print(result.age , result.sex , result.pop2008)

# http://docs.sqlalchemy.org/en/latest/core/sqlelement.html#module-sqlalchemy.sql.expression

#another example
stmt = select([census])                               # Create a query for the census table: stmt
stmt = stmt.where(census.columns.state.in_(states))   # Append a where clause to match all the states in_ the list states
for result in connection.execute(stmt):               # Loop over the ResultProxy & print state and  pop2000
    print(result.state, result.pop2000)

stmt = stmt.where(         
    and_(census.columns.state == 'California',                # The state of California with a non-male sex
         census.columns.sex != 'M'
         )
         
### Order by   (reverse oder by desc()  )
stmt = select([census.columns.state])
stmt = stmt.order_by(census.columns.state)
results = connection.execute(stmt).fetchall()
print(results)              # will now be sorted

## Multiple columns 
stmt = select([census.columns.state , scensus.columns.sex])
stmt = stmt.order_by( census.columns.state , census.columns.sex )

stmt = select([census.columns.state, census.columns.age])             # Build a query to select state and age: stmt
stmt = stmt.order_by(census.columns.state, desc(census.columns.age) ) # Append order by to ascend by state and descend by age
results = connection.execute(stmt).fetchall()                  # Execute the statement and store all the records: results
print(results[:20])                                            # Print the first 20 results

### Decending Order
from sqlalchemy import desc
stmt = select([census.columns.state])                  # Build a query to select the state column: stmt
rev_stmt = stmt.order_by(desc(census.columns.state))   # Order stmt by state in descending order: rev_stmt
rev_results = connection.execute(rev_stmt).fetchall()  # Execute the query and store the results: rev_results
print(rev_results[:10])                                # Print the first 10 rev_results

###### Aggregation functions
from sqlalchemy import func                           # don't import sum function directly, it will confilct with native sum
stmt = select([func.sum(census.columns.pop2008)])
results = connection.execute(stmt).scalar()
print(results)             # 302876613

### GroupBy
# sex
stmt = select([census.columns.sex , func.sum(census.columns.pop2008)])
stmt = stmp.group_by(census.columns.sex)
results = connection.execute(stmt).fetchall()
print(results)             

# sex and age
stmt = select([census.columns.sex , census.columns.age , func.sum(census.columns.pop2008)])
stmt = stmp.group_by(census.columns.sex , census.columns.age )
results = connection.execute(stmt).fetchall()
print(results)             

# column nanmes for functions ... often func_#   .. unless useing label() method
print(results[0].keys())            # ['sex', u'sum_1']      from first groupby example
stmt = select([census.columns.sex , func.sum(census.columns.pop2008).label('pop2008_sum'])
stmt = stmp.group_by(census.columns.sex)
results = connection.execute(stmt).fetchall()
print(results[0].keys())            # ['sex', 'pop2008_sum']      

#### ResultProxy calls:   .fetchall()  .first()   .scalar() 
### Count , count.distinct()
# eg     select([func.count(census.columns.pop2008.distinct())])

stmt = select([func.count(census.columns.state.distinct())])
distinct_state_count = connection.execute(stmt).scalar()
print(distinct_state_count)

## group by state, 
from sqlalchemy import func
stmt = select([census.columns.state , func.count(census.columns.age)])
stmt = stmt.group_by(census.columns.state)
results = connection.execute(stmt).fetchall()
print(results)
print(results[0].keys())

### Highlight using a variable to pop in for the function/select , especially with label.
from sqlalchemy import func
pop2008_sum = func.sum(census.columns.pop2008).label('population')
stmt = select([census.columns.state, pop2008_sum ])
stmt = stmt.group_by(census.columns.state)
results = connection.execute(stmt).fetchall()
print(results)
print(results[0].keys())         # ['state', 'population' ]

### using Dataframes and pandas
import pandas as pd
import matplotlib.pyplot as plt
df = pd.DataFrame(results)
df.columns = results[0].keys()
print(df)
df[10:20].plot.barh()
plt.show()

### Calculations
stmt = select([census.columsn.age , census.clumns.pop2008-census.columns.pop2000).label('pop_change)  ])
stmt = stmt.group_by(census.columns.age)
stmt = stmt.order_by(desc('pop_change'))
stmt = stmt.limit(5)           # limits for top 5 results
results = connection.execute(stmt).fetchall()
print(results)


### Case statements for conditional calcuations
stmt = select([
    func.sum(
        case([
            (census.columns.state == 'New York',
             census.coulumns.pop2008)
        ], else_=0))])
results = connection.execute(stmt).fetchall()
print(results)


#### Cast Statement (int -> float for division ; strings -> dates and times
### Percentage example
from sqlalchemy import case, cast , Float
stmt= select([
    (func.sum(
        case([
            (census.columns.state == 'New York',
             census.columns.pop2008)
        ], else_0)) /
    cast(func.sm(census.columns.pop2008),
        Float) *100).label('nypercent')])
results= connection.execute(stmt).fetchall()
print(results)      #[(Decimal('6.4267619765'),)]
        
  
#### MySQL example  (perhaps use pymysql )
driverdialect = 'mysql+pymysql://'
userpass = 'username:password'
hostport = '@host:port/'
database = 'database_name'
'mysql+pymysql://student:datacamp@courses.csrrinzqubik.us-east-1.rds.amazonaws.com:3306/census'

# Build query to return state names by population difference from 2008 to 2000: stmt
stmt = select([census.columns.state , (census.columns.pop2008-census.columns.pop2000).label('pop_change')])
stmt = stmt.group_by(census.columns.state)            # Append group by for the state: stmt
stmt = stmt.order_by(desc('pop_change'))              # Append order by for pop_change descendingly: stmt
stmt = stmt.limit(5)                                  # Return only 5 results: stmt
results = connection.execute(stmt).fetchall()         # Use connection to execute the statement and fetch all results
for result in results:                                # Print the state and population change for each record
    print('{}:{}'.format(result.state, result.pop_change))

# import case, cast and Float from sqlalchemy
from sqlalchemy import case, cast, Float
# Build an expression to calculate female population in 2000
female_pop2000 = func.sum(
    case([
        (census.columns.sex == 'F', census.columns.pop2000)
    ], else_=0))
total_pop2000 = cast(func.sum(census.columns.pop2000), Float)  # Cast an expression to calculate total population in 2000 to Float
stmt = select([female_pop2000 / total_pop2000 * 100])   # Build a query to calculate the percentage of females in 2000: stmt
percent_female = connection.execute(stmt).scalar()      # Execute the query and store the scalar result: percent_female
print(percent_female)                                   # Print the percentage , 51.0946743229


##### Automatic joins       (some keys predefined in database   :  census.state <==> state_fact.name
stmt = select([census.columns.pop2008, state_fact.columns.abbreviation])         # note, one non-key column from each table
results = connection.execute(stmt).fetchall()
print(results)

# Normally join clauses are right after select, and before any where() order_by() or group_by() , but not needed if 
# it is predefined and available via reflection

stmt = select([func.sum(census.columns.pop2000)])
stmt = stmt.select_from(census.join(state_fact))            #   needs predefined relationship?
stmt = stmt.where(state_fact.columns.circuit_court == '10')
result = connection.execute(stmt).scalar()
print(result)                           # 14945252

### when the relationship is not predefined, must specify   .join(object2join, matchrelationship)
stmt = select([func.sum(census.columns.pop2000)])
stmt = stmt.select_from(census.join(state_fact ,  census.columns.state == state_fact.columns.name ))
stmt = stmt.where( staet_fact.columns.census_division_name == 'East Sout Centeral')
result = connection.execute(stmt).scalar()
print(result)                #16

# Loop over the keys in the result object and print the key and value   (note, used execute(stmt).first()
for key in result.keys():
    print(key, getattr(result, key))


#### Another example
stmt = select([census, state_fact])                    # Build a statement to select the census and state_fact tables: stmt

# Add a select_from clause that wraps a join for the census and state_fact
# tables where the census state column and state_fact name column match
stmt = stmt.select_from(  census.join(state_fact, census.columns.state == state_fact.columns.name))

result = connection.execute(stmt).first()                # Execute the statement and get the first result: result
for key in result.keys():                        # Loop over the keys in the result object and print the key and value
    print(key, getattr(result, key))

# Build a statement to select the state, sum of 2008 population and census  division name: stmt
stmt = select([
    census.columns.state ,
    func.sum(census.columns.pop2008) ,
    state_fact.columns.census_division_name
])

# Append select_from to join the census and state_fact tables by the census state and state_fact name columns
stmt = stmt.select_from(
    census.join(state_fact, census.columns.state == state_fact.columns.name)
)
stmt = stmt.group_by(state_fact.columns.name)          # Append a group by for the state_fact name column
results = connection.execute(stmt).fetchall()          # Execute the statement and get the results: results
for record in results:                                 # Loop over the the results object and print each record.
    print(record)


#### Hierarchial Tables (self referential tables)     .... use alias() method for temp rename
# Eg, Employees ... wehre manager column has and id number that refers to a row in the same table of employees

managers = employees.alias()                            # managers points to employees, but now has unique name
stmt = select(
    [managers.columns.name.label('manager') ,
     employees.columns.name.label('employee')           # if using a function on a column, do so on the non-alias table
    ])
stmt = stmt.select_from( employees.join(managers, 
    managers.columns.id == employees.columns.manager
    )
stmt = stmt.order_by(managers.columns.name)             # note that we group by the table made in the alias
print(connection.execute(stmt).fetchall())

### important to target group_by() at the right alias

### Example
managers = employees.alias()                            # Make an alias of the employees table: managers
stmt = select(                                          # Build a query to select manager's and their employees names
    [managers.columns.name.label('manager'),
     employees.columns.name.label('employee')]
)
stmt = stmt.where(managers.columns.id == employees.columns.mgr)  # Match managers id with employees mgr: stmt
stmt = stmt.order_by(managers.columns.name)                       # Order the statement by the managers name: stmt
results = connection.execute(stmt).fetchall()
for record in results:
    print(record)
*** Produces:
SELECT employees_1.name AS manager, 
        employees.name AS employee 
FROM employees AS employees_1, employees 
WHERE employees_1.id = employees.mgr 
ORDER BY employees_1.name



managers = employees.alias()
stmt = select([managers.columns.name, func.count(employees.columns.name)])
stmt = stmt.where(managers.columns.id == employees.columns.mgr)
stmt = stmt.group_by(managers.columns.id)
results = connection.execute(stmt).fetchall()
for record in results:
    print(record)

SELECT employees_1.name, 
       count(employees.name) AS count_1 
FROM employees AS employees_1, employees 
WHERE employees_1.id = employees.mgr 
GROUP BY employees_1.name


###### Dealing with lots of results in the ResultSet  ... just take chunks
while more_results:
    partial_results = results_proxy.fetchmany(50)
    if partial_resutls==[]:
        more_results = False
        
    for row in partial_results:
        state_count[row.state] += 1
    else: 
        state_count[row.state = 1
results_proxy.close()
















##### Remote Import

after going through the source code of PyHive, I understood that pip install PyHive install a different code than the one 
at https://github.com/dropbox/PyHive (even though the version are the same)
1) so I uninstalled previous version of pyhive
2) downloaded the souce from https://github.com/dropbox/PyHive and installed this
3) now im able to connect to remote hive using username/password
---- code -----
from pyhive import hive
conn = hive.Connection(host="<host>", port=10000, username="<username>", password="<your pwd>", auth='CUSTOM')
cur = conn.cursor()
cur.execute('select * from <table>');
print cur.fetchone()

ibm-db, the official DB2 driver for Python and Django is here:

https://code.google.com/p/ibm-db/
Here's a recent tutorial for how to install everything on Ubuntu Linux:

http://programmingzen.com/2011/05/12/installing-python-django-and-db2-on-ubuntu-11-04/
I should mention that there were several older unofficial DB2 drivers for Python. ibm-db is the one you should be using.

-------------------------------------------------
The documentation is difficult to find, and once you find it, it's pretty abysmal. Here's what I've found over the past 3 hours.

You need to install ibm_db using pip, as follows:

pip install ibm_db
You'll want to create a connection object. The documentation is here.

Here's what I wrote:

from ibm_db import connect
# Careful with the punctuation here - we have 3 arguments.
# The first is a big string with semicolons in it.
# (Strings separated by only whitespace, newlines included,
#  are automatically joined together, in case you didn't know.)
# The last two are emptry strings.
connection = connect('DATABASE=<database name>;'
                     'HOSTNAME=<database ip>;'  # 127.0.0.1 or localhost works if it's local
                     'PORT=<database port>;'
                     'PROTOCOL=TCPIP;'
                     'UID=<database username>;'
                     'PWD=<username password>;', '', '')
Next you should know that commands to ibm_db never actually give you results. 
Instead, you need to call one of the fetch methods on the command, repeatedly, to get the results. 
I wrote this helper function to deal with that.

def results(command):
    from ibm_db import fetch_assoc

    ret = []
    result = fetch_assoc(command)
    while result:
        # This builds a list in memory. Theoretically, if there's a lot of rows,
        # we could run out of memory. In practice, I've never had that happen.
        # If it's ever a problem, you could use
        #     yield result
        # Then this function would become a generator. You lose the ability to access
        # results by index or slice them or whatever, but you retain
        # the ability to iterate on them.
        ret.append(result)
        result = fetch_assoc(command)
    return ret  # Ditch this line if you choose to use a generator.
Now with that helper function defined, you can easily do something like get the 
information on all the tables in your database with the following:

from ibm_db import tables

t = results(tables(connection))
If you'd like to see everything in a given table, you could do something like this now:

from ibm_db import exec_immediate

sql = 'LIST * FROM ' + t[170]['TABLE_NAME']  # Using our list of tables t from before...
rows = results(exec_immediate(connection, sql))
And now rows contains a list of rows from the 170th table in your database, where every row contains a dict of column name: value.

Hope this all helps
---------------------------------------
https://www.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.swg.im.dbclient.python.doc/doc/t0054367.html
Before you can connect to an IBMÂ® database server and run SQL statements, you must set up 
the Python environment by installing the ibm_db (Python) driver and, optionally, the 
ibm_db_sa (SQLAlchemy) or ibm_db_django (Django) adapter.
*Ensure that the following software is installed on your system:
**Python 2.5 or later. For Linux operating systems, you also require the python2.5-dev package.
**The setuptools program or the distribute program. The setuptools program is available at 
   http://pypi.python.org/pypi/setuptools, and the distribute program is available at 
   http://pypi.python.org/pypi/distribute. You can use the setuptools program or the distribute program to 
   download, build, install, upgrade, and uninstall Python packages.
If your Python application will connect to a remote IBM database, one of the following products on the 
  computer where your application will run:
The IBM Data Server Client product
The IBM Data Server Runtime Client product
The IBM Data Server Driver Package product
The IBM Data Server Driver for ODBC and CLI product
If your Python application connects to an IBM database server on the local computer, no additional 
 IBM data server products are required.

#####Procedure
To set up the Python environment:

Using one of the following two methods, install the ibm_db Python driver:
Install from the remote repository:
Set the IBM_DB_HOME environment variable by using the export command:
$export IBM_DB_HOME=DB2HOMEcopy to clipboard
where DB2HOME is the directory where the IBM data server product is installed.
For example, issue the following command to set the IBM_DB_HOME environment variable:
$ export IBM_DB_HOME=/home/db2inst1/sqllibcopy to clipboard
Issue the following command:
$ easy_install ibm_db copy to clipboard
Use the files that are included with the IBM data server products. The IBM data server client and IBM Data Server Driver Package software include the required Python files.
Change the current path to the IBM data server product installation path where the Python egg files are located (IBM_DB_HOME/pythonXX).
Issue the following command:
$ easy_install ibm_db-X.X.X-pyX.X-XXXXXXXX.eggcopy to clipboard
Optional: Using one of the following two methods, install the ibm_db_sa SQLAlchemy adapter or the ibm_db_django Django adapter:
Install from the remote repository:
To install the SQLAlchemy adapter, issue the following command:
$ easy_install ibm_db_sacopy to clipboard
To install the django adapter, issue the following command:
$ easy_install ibm_db_djangocopy to clipboard
Use the files that are included with the IBM data server products:
Change the current path to the IBM data server product installation path where the Python egg files are located (IBM_DB_HOME/pythonXX).
To install the SQLAlchemy adapter, issue the following command:
$ easy_install ibm_db_sa-X.X.X-pyX.X.eggcopy to clipboard
To install the django adapter, issue the following command:
$ easy_install ibm_db_django-X.X.X-pyX.X.eggcopy to clipboard
Ensure that the Python driver can access the libdb2.so CLI driver file:
For 32-bit Linux and UNIX operating systems other than the AIX operating system, set the LD_LIBRARY_PATH variable to the IBM_DB_HOME/lib32 directory by issuing the export command:
export LD_LIBRARY_PATH=IBM_DB_HOME/lib32copy to clipboard
For 64-bit Linux and UNIX operating systems other than the AIX operating system, set the LD_LIBRARY_PATH variable to the IBM_DB_HOME/lib64 directory by issuing the export command:
export LD_LIBRARY_PATH=IBM_DB_HOME/lib64copy to clipboard
For a 32-bit AIX operating system, set the LIBPATH variable to theIBM_DB_HOME/lib32 directory by issuing the export command:
export LIBPATH=IBM_DB_HOME/lib32copy to clipboard
For a 64-bit AIX operating system, set the LIBRARY_PATH variable to the IBM_DB_HOME/lib64 directory by issuing the export command:
export LIBPATH=IBM_DB_HOME/lib64



https://www.ibm.com/support/knowledgecenter/en/SSEPGG_9.7.0/com.ibm.swg.im.dbclient.python.doc/doc/t0054368.html
ibm_db.connect
ibm_db.pconnect          # creates persistent connection
Example 1: Connect to a local or cataloged database

import ibm_db
conn = ibm_db.connect("database","username","password")copy to clipboard
Example 2: Connect to an uncataloged database

import ibm_db
ibm_db.connect("DATABASE=name;HOSTNAME=host;PORT=60000;PROTOCOL=TCPIP;UID=username;
                PWD=password;", "", "")

After connecting to a database, use functions available in the ibm_db API to prepare and execute SQL statements. The SQL statements can contain static text, XQuery expressions, or parameter markers that represent variable input.

### Preparing and executing a single SQL statement in Python
To prepare and execute a single SQL statement, use the function:    ibm_db.exec_immediate 

###Preparing and executing SQL statements with variable input in Python
To prepare and execute an SQL statement that includes variable input, use the functions:
    ibm_db.prepare, ibm_db.bind_param, and ibm_db.execute 
Preparing a statement improves performance because the database server creates an 
optimized access plan for data retrieval that it can reuse if the statement is executed again.













