PEP8 , by Guido van Rossum, BDFL
PEP20 Tim Peters, 20 aphorisms:   >import this
#### Magic command
!ls

filename = 'huck_finn.txt'
file = open(filename , mode='r')    # 'r' is to read
file = open(filename , mode='w')    # 'w' is to read
text = file.read()
file.close()
print(text)

# Context managers will automatoicall close files, and bind variable with the open statement
with open(filename, mode='r') as file:
    print(file.read())
    
# Read & print the first 3 lines
with open('moby_dick.txt') as file:
    print(file.readline())
    print(file.readline())
    print(file.readline())
    

# row , record 
# columns =fields or attributes or features

########## Using Numpy ########## loadtxt() , genfromtxt() , recfromcsv()  
Numpy can import arrays of all types, or can auto-interpret into structured array (genfromtxt/recfromcsv)

import numpy as np
filename= 'MNIST.txt'
data=np.loadtxt(filename, delimiter=',')  # delimiter is whitespace by default
data=np.loadtxt(filename, delimiter=',' , skiprows=1, usecols=[0,2])
data=np.loadtxt(filename, delimiter=',' , skiprows=1, usecols=[0,2], dtype=str)  # import all data as str
data=np.loadtxt(filename, delimiter=',' , skiprows=1, usecols=[0,2], dtype=float)  # all data are floats
print(data[9])   # Prints 10th line

#mixed data types  structured array... 1D arraw , where each element of arraw is a row of the flat file
data = np.genfromtxt(filename, delimiter=',', names=True, dtype=None)    # names=True if header, dtype=None for auto

file = 'titanic.csv'
data=np.recfromcsv(file)  #  dtype=None, delimiter=',' by default
print(data[:3])   # prints out first three enteries of d

##### Example Import package
import numpy as np

# Assign filename to variable: file
file = 'digits.csv'

# Load file as array: digits
digits = np.loadtxt(file, delimiter=',')

# Print datatype of digits
print(type(digits))

# Select and reshape a row
im = digits[21, 1:]
im_sq = np.reshape(im, (28, 28))

# Plot reshaped data (matplotlib.pyplot already loaded as plt)
plt.imshow(im_sq, cmap='Greys', interpolation='nearest')
plt.show()

# Assign filename: file
file = 'seaslug.txt'

###### Example Import file: data
data = np.loadtxt(file, delimiter='\t', dtype=str)

# Print the first element of data
print(data[0])

# Import data as floats and skip the first row: data_float
data_float = np.loadtxt(file, delimiter='\t', dtype=float, skiprows=1)

# Print the 10th element of data_float
print(data_float[9])

# Plot a scatterplot of the data
plt.scatter(data_float[:, 0], data_float[:, 1])
plt.xlabel('time (min.)')
plt.ylabel('percentage of larvae')
plt.show()

########## Using Pandas ##########
# standard best practice to put data into data frames with pandas
import pandas as pd
filename = 'wine-quality.csv'
data = pd.read_csv(file)   # delimiter default=,   header=True by default
data2 = pd.read_csv(file, nrows=5, header=None)   
data3 = pd.read_csv(file, sep='\t' , comment='#', na_values='Nothing')
data.head()
data_array=data.values          # transfers dataframe into numpy array
print(type(data_array)

pd.DataFrame.hist(data[['Age']])
plt.xlabel('Age (years)')
plt.ylabel('count')
plt.show()

#### keep eye on Feather , made by HadleyWickham and wesmckinney

##### Pickled files  (serialized)
import pickle
with open('pickled_fruit.pkl', 'rb') as file:       # 'rb' read, binary
    data=pickle.load(file)
print(data)
print(type(data))

##### Excel Sheets
import pandas as pd
file='urbanpop.xls'
data= pd.ExcelFile(file)
print(data.sheet_names)
df1 = data.parse('sheetname')   # text string for sheet
df2 = data.parse(0)             # sheet index [0,1,2, .. etc
df3 = data.xl.parse(0,skiprows=1, names=['Country', 'AAM'])        # parse 1st sheet  and rename the columns
df4 = data.parse(0,parse_cols[0], skiprows=1, names=['Country'])   # parse 1st column, rename to country
print(df2.head())

##### SAS / Stata 
import pandas as pd
from sas7bdat import SAS7BDAT
with SAS7BDAT('urbanpop.sas7bdat') as file:
    df_sas=file.to_data_frame()
pd.DataFrame.hist(df_sas[['P']])         # Print a histogram of one column of the DataFrame
plt.ylabel('count')
plt.show()

import pandas as pd
df = pd.read_stata('urbanpop.dta')
print(df.head())
pd.DataFrame.hist(df[['disa10']])        # Print a histogram of one column of the DataFrame
plt.xlabel('Extent of disease')
plt.ylabel('Number of countries')
plt.show()

##### HDF5 files     ..can scale to exabytes
import h5py
filename = 'H-H1_LOSC_4_V1-81541100-4096.hdf5'
data= h5py.File(filename, 'r')   # 'r' for read
print(type(data)

for key in data.keys():
    print(key)                   #  meta , quality , strain
print(type(data['meta']))        # <class 'h5py._hl.group.Group'>
for key in data['meta'].keys()
    print(key)                   # Description , DescritptionURL , Detector , Duration ....
print(data['meta']['Description'].value,data['meta']['Detector'].value)   # b'Strain data'

group = data['strain']          # Get the HDF5 group: group
for key in group.keys():        # Check out keys of group
    print(key)

# Example
strain = data['strain']['Strain'].value     # Set variable equal to time series data: strain
num_samples = 10000                         # Set number of time points to sample: num_samples
time = np.arange(0, 1, 1/num_samples)       # Set time vector
plt.plot(time, strain[:num_samples])        # Plot data
plt.xlabel('GPS Time (s)')
plt.ylabel('strain')
plt.show()


### Matlab files ... .mat files  containers for various objects
scipy.io.loadmat()       scipy.io.savemat()  

import scipy.io
filename = 'workspace.mat'
mat = scipy.io.loadmat(filename)
print(type(mat))            # <class 'dict'>
print((type(mat['x'}))      # <class 'numpy.ndarray'>

print(mat.keys())
print(type(mat['CYratioCyt']))         # print the type corresponding to key CYratioCyt
print(np.shape(mat['CYratioCyt']))     # print the shape of hte value 
data = mat['CYratioCyt'][25, 5:]
fig = plt.figure()
plt.plot(data)
plt.xlabel('time (min.)')
plt.ylabel('normalized fluorescence (measure of expression)')
plt.show()


########## Relational Databases ##########    sqlite3  sqlalchemy       ... Todd's 12 Rules (0indexed = 13)
#####  PostgreSQL , SQLite , mysql , DB2 , SASql , HiveSQL , Structured Query Language
from sqlalchemy import create_engine
engine = create_engine('sqlite:///Northwind.sqlite')
table_names = engine.table_names()
print(table_names)

SELECT * FROM Table_Name    (Hello World of SQL)

from sqlalchemy import create_engine
import pandas as pd
engine = create_engine('sqlite:///Northwind.sqlite')

con = engine.connect()
rs = con.execute("SELECT * FROM Orders")
df = pd.DataFrame(rs.fetchall())
df.columns = rs.keys()
con.close()

with engine.connect() as con:
    rs = con.execute("SELECT OrderId, OrderDate, FROM Orders")
    df = pd.dataFrame(rs.fecthmany(size=5))
    df.columns = rs.keys()
print(len(df))


## import with pandas
sqltext ="SELECT Title , Name FROM Album INNER JOIN Artist ON Album.ArtistID = Artist.ArtistID WHERE Mills<250000"
df = pd.read_sql_query(sqltext, engine)




########################## Web Data #######

from urllib.request import urlretrieve                            # similar to urlopen() , but accepts URLs
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
urlretrieve(url, 'winequality-white.csv')           # note, saves the .csv localy  (makes GET request, and local save)


from urllib.request import urlretrieve
import pandas as pd
url='https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'
urlretrieve(url,'winequality-red.csv')
df = pd.read_csv('winequality-red.csv', sep=';')       # Read local file into a DataFrame and print its head
print(df.head())

df = pd.read_csv(url, sep=';')                         # Read url directly into a DataFrame and print its head
print(df.head())

pd.DataFrame.hist(df.ix[:, 0:1])                             # Plot first column of df
plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')
plt.ylabel('count')
plt.show()



#### Importing url to panda dictionary without local data save , even with excel data
import pandas as pd                    # Import package
url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'
xl = pd.read_excel(url,sheetname=None)  # Read in all sheets of Excel file: xl , sheets are keys
print(type(xl))                         # <class 'dict'>
print(xl.keys())                        # Print the sheetnames to the shell
print(xl['1700'].head())                # Print the head of the first sheet (using its name, NOT its index)



##### getting HTTP GET
from urllib.request import urlopen, Request
url = 'https://www.wikipedia.org/'
reqeust = Reqeust(url)
response = urlopen(request)
html = response.read()
print(type(html))
response.close()

# better automated using the Requests package
import requests
url = 'https://www.wikipedia.org/'
r= reqeusts.get(url)
text = r.text


from bs4 import BeautifulSoup
import requsts
url = 'https://www.crummy.com/software/BeautifulSoup/'
r= requsts.get(url)
html_doc = r.text
soup = BeautifulSoup(html_doc)
print(soup.title)
print(soup.get_text())
a_tags = soup.find_all('a')
for link in a_tags:                 # Print the URLs to the shell
    print(link.get('href'))

for link in soup.find_all('a'):      # shortcut
    print(link.get('href'))


pretty_soup = BeautifulSoup.prettify(soup)


#####  JSON (JavaScript Object Notation (Douglas Crokford) .. human readable real-time server-to-browser communication
import json
with open('snakes.json' , 'r') as json_file:
    json_data = json.load(json_file)
print(type(json.data))
for k in json_data.keys():
    print(k + ': ', json_data[k])    


import requests
url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'
url = 'http://www.omdbapi.com/?t=hackers&apikey=ff21610b'      # ? is a query string .. text follwoing is the query
url = 'http://www.omdbapi.com/?apikey=ff21610b&t=the+social+network'          # note, apikey needs to be included sometimes
r= reqeusts.get(url)
json_data = r.json()                # built in json method!
for key, value in json_data.items():
    print(key + ':' , value)

# some json reults are nested jsons ... like wikipedia on pizza
pizza_extract = json_data['query']['pages']['24768']['extract']
print(pizza_extract)






