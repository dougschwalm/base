
################################################################################
###############################  Infile Imports ################################
################################################################################

# To support both python 2 and python 3
from __future__ import division, print_function, unicode_literals

# Common imports
import numpy as np
import os
import pandas as pd
#import sklearn.linear_model
# from sklearn import linear_model
import sklearn as skl
import matplotlib as mpl
import matplotlib.pyplot as plt

# Ignore useless warnings (see SciPy issue #5998)
import warnings
warnings.filterwarnings(action="ignore", message="^internal gelsd")

################################################################################
###############################  Infile OS defs  ###############################
################################################################################


# Project Directory Structure
DIR_PROJECT_ROOT = "~/Projects/"
DIR_P_IMAGE = os.path.join(DIR_PROJECT_ROOT, "Images")
AUTHOR_NAME = Douglas D Schwalm"

datapath = os.path.join("first", "second", "")


################################################################################
##############################  HyperParameters  ###############################
################################################################################
# set random seeds
np.random.seed(89)
seedsplit = 42


################################################################################
##############################  In-file Scripts  ###############################
################################################################################

# Categorization Labels
DIRECT_MAIL = "dm"

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(DIR_P_IMAGE, "gr_", DIRECT_MAIL , fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

# for ownuse only ... use sklearn train_test_split()
def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]





################################################################################
############################# In-file Formatting ###############################
################################################################################

# To plot pretty figures
%matplotlib inline
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

################################################################################
################################## Load Data  ##################################
################################################################################

indata1 = pd.read_csv(datapath + "filename.csv" , thousands=',')
indata1.head(3)
indata1.info()
indata1.describe()

indata2 = pd.read_csv(datapath + "filename.csv" , thousands=','
                      encoding='latin1', na_values="n/a")
indata.rename(columns={"old name": "NewName"}, inplace=True)
indata.set_index("uniq_id", inplace=True)            # sets index to a data column                     
indata2.head(3) 


# removing selected outliers
remove_indicies = [0,1,6,8,33,34,35]
keep_indicies = list(set(range(36)) - set(remove_indices)) 




sortvar = "sort_variable" 
alldata= pd.merge(left=indata1 , right=indata2, left_index=True, right_index=True)
alldata.sort_values(by=sortvar, inplace=True)

################################################################################
############################### Visualize Data  ################################
################################################################################

# Visualize the data
xvar="x-variable"
yvar="y-variable"
alldata.plot(kind='scatter' x=xvar , y=yvar)
plt.axis([0,100, 0, 100])
plt.text(90,90, r"Title $\alpha_1$ ", fontsize=14, color="b")
plot.legend(loc="lower right")
save_fig('newgraph')
plt.show()

alldata["var1"].hist()

alldata.hist(bins=50, figsize=(20,15))
save_fig("attribute_histogram_plots")
plt.show()


################################################################################
##################################### EDA ######################################
################################################################################

corr_matrix = alldata.corr()
corr_matrix["var1"].sort_values(ascending=False)

contvars = ["cvar1", "cvar2"]
pd.scatter_matrix(housing[attributes], figsize=(12,8)) 

# cycle through all variables in the data frame
for var in alldata.columns:
    alldata[var].hist()
    
varnames = list(alldata.columns.values)
varnames2 = list(alldata.columns.values.tolist())


################################################################################
################################# Report Data  #################################
################################################################################

specific_val = indata2.loc["xval"]["varname"]

########## Categorical Data
alldata["cvar1"].value_counts()

################################################################################
##############################  Split Data ##############################
################################################################################

# for ownuse only ... use sklearn train_test_split()
# train_set , test_set = split_train_test(alldata, 0.2)
# zlib crc32    .. hashlib ...

train_set , test_set skl.train_test_split(alldata, test_size=0.2, randomstate=seedsplit)
print(len(train_set, "obs in training, + " , len(test_set), " obs in test set")


######  stratified split
alldata["var_cat"] = pd.cut(alldata["var"],
                    bins=[0., 1.5, 3.0, 4.5, 6. np.inf],
                    labels=[1,2,3,4,5])

split = skl.StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seedsplit)
for train_index, test_index in split.split(alldata, alldata["var_cat"]):
  strat_train_set = alldata.loc[train_index]
  strat_test_set = alldata.loc[test_index]
  
strat_test_set["var_cat"].value_counts() / len(strat_test_set)
alldata["var_cat"].value_counts() / len(alldata)




################################################################################
##############################  Linear Regression ##############################
################################################################################
# Train model
model = skl.linear_model.LinearRegression()
model.fit(X,y)
b0, b1 = model.intercept_[0] , model.coef_[0][0]

##########
# Make a prediction
X_new = [[100]]  # some input value
print(model.predict(X_new))  # will output the predicted value


poly = skl.preprocessing.PolynomilaFeatures(degree=60, include_bias=False)
scalar = skl.preprocessing.StandardScalar()
linreg2 = skl.linear_model.LinearRegression() 
pipeline_reg = skl.pipeline.Pipeline([('poly', poly), ('scal', scaler), ('lin', linreg2) ] )
pipeline_erg.fit(Xfull, yfull)
curve=pipeline_erg.predict(X[:, np.newaxis])
plt.plot(X,curve)
save_fig('overfitting_model_plot')
plot.show()

################################################################################
############################## k-Nearest Neighbors #############################
################################################################################

model = skl.neighbors.KNeighborsRegressor(n_neighbors=3)









