# from pluralsight , pavel kordex, pandas-fundamentals

import pandas as pd
import os
import json
import matplotlib.pyplot as plt
from matplotlib import rcParms

# setting a path
filename = os.path.join('..', 'directory', 'filename' )

# read data from csv into dataframe
df = pd.read_csv('file_name.csv') 
df = pd.read_csv(filename)                     # alternative way using predefined filename

df = pd.read_csv(filename , index_col='keyvar' )     # will use keyvar as the index instead of autogenerate

# subset some rows (top) 
df = pd.read_csv(filename , nrows=5) 

# subset some columns (inclusive)
keepcols = [ 'var1' , 'var2' , 'var3' ]
df = pd.read_csv(filename , usecols=keepcols )          
df = pd.read_csv(filename , usecols=[ 0 , 1, 4, 5 ] )    # column postions fine too ... can't mix          


# ####################  Reading "from_records"   methods 
#  Json folders to data frame
# suppose data are in 7000 files xxxx.json  in object records

pd.DataFrame.from_records(records,columns=["Coltitle1", "Coltitle2"]  #  will apply col titles to columns

# create functions to read one file
KEYS_TO_USE = [ 'var1' , 'var2' , var3' ]

def get_record_from_file(file_ath , keys_to_use):
   """ Process single json file and return a tuple containing specific fields."""
   
   with open(file_path) as filename:
      content = json.load(filename)
      
      record = []
      for field in keys_to_use:
         record.append(content[field])
         
      return tuple(record)
      
# iterate over files
def read_artworks_from_json(keys_to_use):
   """ Traverse the directories with JSON files.
   For first file in eachdirectory call fucntion for processing single file and go to the next directry.
   """
   
   JSON_ROOT = os.path.join('..', 'directory' , 'filename' )
   
   artworks = []
   for root, _, files in os.walk(JSON_ROOT):
      for f in files:
         if f.endswith('json'):
            record = get_record_from_file(
                         os.path.join(root, f),
                         keys_to_use)
            filename.append(record)
         break                                    # visit only 1st file in the directory
 df = pd.Dataframe.from_records(filename,
                                columns=keys_to_use
                                index="id")
 return df
   



# save data for later
df.to_pickle(os.path.join('..', 'data_frame..pickle'))

# read data into a data frame
df = pd.read_pickle(os.path.join('..', 'data_frame.pickle'))


# create a list of items from one column (for maniuplation .. not for data tranfrom)
listname = df['colname']             # will put elements of colname into list name

pd.unique(listname)                  # will cull only unique values of listname
len(pd.unique(listname))             # counts unique instances of values in listname


# Filtering
df['colname'] == "value"              # creates a boolian column, allows for filtering of rows by colvals
s = df['colname'] == "value"          # creates the data filtered by value
s.value_counts()                      # gives number of rows in the filtered set

# Frequency data
freq_counts = df['colname'].value_counts()        # counts number of row instances per colname value
freq_counts['col_value']                          # returns number of rows that had col == "Value"


# checking the type of an element
type(df['height'])    # will show that it is an object, like most of panda data type

# sorting a data column to check for dirty data
df['colname'].sort_values().head()
df['colname'].sort_values().tail()

#converting types
pd.to_numeric(df['colname'])
pd.to_numeric(df['colname'], errors='coerce')  #force NaNs
# to update the data in the data frame in one step:
df.loc[:, 'colname'] = pd.to_numeric(df['colname'], errors='coerce') 


# creating new column in data frame (as product of to other variables)
newcol = df['var1'] * df['var2']
df = df.assign(newcol=newcol


# taking a slice of a data frame:  loc iloc

df['colname']                       # shortcut for just one column
df.colname                          # avoid this, can have unexpected results
df[ list ]                          # this is fine .. eg:  df['colname1' , 'colname2' ]


df.loc[1035,'artist']               # row index=1035, column name = 'artist

df.iloc[0,0 ]                       #  first element in first row of first column
df.iloc[0,:]                        # first row
df.iloc[:,0:2]                      # first columns 

df.loc[df['colname'] == 'textstring1' , 'textsring2' ]   # also ok


df['colname'].max()     # returns max value
df['colname'].idxmax()  # returns rowlabel
df.loc[df['colname'].idxmax(), : ] # returns row with max value of colname 

# Copying a part of a data frame
small_df = df.iloc[49980:50019, :].copy()


# iterating over a group
df.groupby('colname'):             # data frame group by ... iterable .. creates smaller data frames by unique values
grouped = df.groupby('colname')

for name, group_df in grouped:
   print(name)
   print(group_df)
   break                            # this will check the process, giving only the first roup
   
# Operations on Groups / Aggregate operations

for name, group_df in small_df.groupby('colname'):
   min_val = group_df['colname2'].min()
   print("{}: {}".format(name, min_val))

# Transformations on Groups

# example, replace NaN with the modal value of the variable
def fill_values(series):
   values_counted = series.value_counts()
   if values_counted.empty:
      return series
   most_frequent = values_counted.index[0]
   new_medium = series.fillna(most_frequent)
   return new_medium
   
def transform_df(source_df):
   groups_dfs = []
   for name, group_df in source_df.groupby('colname'):
      filled_df = group_df.copy()
      filled_df.loc[:, 'missvalcolname2'] = fill_values(group_df['missvalcolname2'])
      group_dfs.append(filled_df)
      
   new_df = pd.concat(group_dfs)
   return new_df
      
 filled_df = transform_df(small_df)
      
# Note, looping is available, but better methods are available, for example, as follows:
grouped_colname = df.groupby('groupingcol')['colname']
df.loc[:, 'colname' ] = grouped_medium.transfrom(fill_values)       # note, this fill_values function was defined above

# another example, to find minimum acuqsitionYaer by artist:
grouped_acq_year = df.groupby('artist')['acquisitionyear']
min_acqusition_years = grouped_acq_year.agg(np.min)
min_acqusition_years = grouped_acq_year.min()       # special case available for min



# Filtering on groups

# look at duplicates
grouped_colvar  = df.groupby('colvar')
condition = lambda x: len(x.index) > 1 
filtered_df = grouped_titles.filter(condition)



#Examples:
grouped_mediums = small_df.groupby('artist')['medium']
smalldf.loc[:, 'medium'] = grouped_mediums.transfrom(fill_values)

# Min
df.groupby('artist').agg(np.min)   # numpy min function ... often best to used numpy  but here
df.groupby('artist').min()         # pandas' min shortcut may be just as good or better

# Filter
grouped_titles = df.groupby('title')
title_counts = grouped_titles.size().sort_values(acending=False)
condition = lambda x: len(x.index) > 1             # appears at least two times in the data set
dup_titles_df = grouped_titles.filter(condition)
dup_titles_df.short_values('title', inplace=True)




########## Output

small_df.to_excel("basic.xlsx")
small_df.to_excel("noindex.xlsx", index=False)
small_df.to_excel("columns.xlsx" , columns=["colname1", "colname2" , "colname3"] )
#multiple worksheets
writer = pd.ExcelWriter('multipe_sheets.xlsx' , engine='xlsxwriter')
small_df.to_excel(writer, sheet_name="Preview", index=False)
df.to_excel(writer, sheet_name="Complete", index=False)
writer.save()

#conditional formatting
artist_counts=df['artist'].value_counts()
artist_counts.head()
writer = pd.ExcelWriter('colors.xlsx', engine='xlsxwrtier')
artist_counts.to_excel(writer, sheet_name="artist Counts")
sheet = sriter.sheets['Artist Counts']
cell_range = 'B2:B{}'.format(len(artist_counts.index))
sheet.condition_format(cells_range, {'type': '2_color_scale',
                                             'min_value': '10',
                                             'min_type': 'percentile',
                                             'max_value': '99',
                                             'max_type' : 'percentile'})
writer.save()


#SQL
import sqlite3

with sqlite3.connect('my_database.db') as conn:
   small_df.to_sql('Tate', conn)
   
# import sqlalchemy as sa
# with sa.create_engine('postgresql://localhost/my_data') as conn:
#   small_df.to_sql('Tate', conn)

# JSON
small_df.to_json('default.json')
small_df.to_json('table.json', orient='table'))          # puts schema first ,then data in squential tables after



#Plotting      (uses matplotlib)
s.plot()
df.plot()

#eg:
aquisition_years = df.groupby('aquisitionYear').size()         # creates 2 variables
aquisition_years.plot()                                        # basic plot, not formatting

#matplotlib  (figure based)
# subplots are colled axes

import matplotlib.pyplot as plt
from matplotlib import rcParms

rcParms.update({'figure.autolayout': True,
                'axes.titlepad': 20})
                
fig= plt.figure()
subplot = fig.add_subplot(1,1,1)
aquisition_years.plot(ax=subplot)
fig.show()
                
# Possible additions:
subplot.set_xlabel("Acquistion Year")
subplot.set_ylabel("aqrtworks Acquired")
subplot.locator_parms(nbins=40, axis='x')
acquisition_years.plot(ax=subplot, rot=45)         # will rotate ticks
acquisition_years.plot(ax=subplot, rot=45, logy=True)         # will create log-scale for y-axis
acquisition_years.plot(ax=subplot, rot=45, logy=True, grid=True)         # puts grid on graph

# set fonts
title_font = {'family': 'source sans pro',
              'color' : 'dark blue' ,
              'weight': 'normal' ,
              'size' : 20,
              }
lables_font = {'family': 'consolas',
              'color' : 'dark red' ,
              'weight': 'normal' ,
              'size' : 16,
              }          




#  Total together:
fig= plt.figure()
subplot = fig.add_subplot(1,1,1)
acquisition_years.plot(ax=subplot, rot=45, logy=True, grid=True)
subplot.set_xlabel("Acquistion Year" , fontdict=labels_font, labelpad=10)       # notice font addition
subplot.set_ylabel("aqrtworks Acquired" , fontdict=labels_font)                 # notice font addition
subplot.locator_parms(nbins=40, axis='x')
subplot.set_title("Tate Gallery Acquisitions", fontdict=title_font)              # sets the title, plus font
fig.show()
fig.savefig('plot.png')
fig.savefig('plot.svg', format='svg')



